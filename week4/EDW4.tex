\documentclass[10pt]{beamer}

\input{../utils/beamerpreamble.tex}


\title[]{Experiment Planning and Design}
\subtitle[]{Lecture 4: Statistical Concepts}
\author[Claus Aranha]{Claus Aranha\\{\footnotesize caranha@cs.tsukuba.ac.jp}}
\institute{Department of Computer Science}
\date{2015-05-12}

\begin{document}

\section{Outline}
\subsection{Outline}

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}
  \frametitle{Notes} 
  \begin{itemize}
  \item Sorry about the sickness last class; Let's try Class 3 again!

    \bigskip

  \item No class on May 19th and May 26th;
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Class Outline}
  \begin{itemize}
  \item Random Variables
  \item Point Estimators
  \item Interval Estimators
  \item Hypothesis Testing
  \end{itemize}
  \begin{exampleblock}{}
    The goal of this class is to allow you to do a simple analysis of
    the data going into, and coming out of an experiment.
  \end{exampleblock}
\end{frame}

\section{Introduction}
\subsection{Introduction}
\begin{frame}
  \frametitle{Introduction: Probability vs Statistics}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{block}{Probability}
      Given the pool, what are the odds of drawing a combination of certain colors?
      \begin{center}
      \includegraphics[height=.35\textheight]{img/ballpool}
      \end{center}
    \end{block}
    \column{0.5\textwidth}
    \begin{block}{Statistics}
      Given the colors of a few balls drawn, what can I know about the pool?
      \begin{center}
      \includegraphics[height=.35\textheight]{img/ballhand}
      \end{center}
    \end{block}
  \end{columns}

  \bigskip

  \structure{Statistical Inference:} Using \emph{samples} to draw
  conclusions about \emph{populations}
\end{frame}

\begin{frame}
  \frametitle{Population, Sample and Observation}
  \begin{columns}
    \column{0.9\textwidth} ``A \structure{population} is a large set
    of objects of a similar nature which is of interest as a
    whole''. It can be an actual set (all balls in the pool), or an
    hypothetical one (all possible outcomes for an experiment).
    \column{0.1\textwidth}
    \includegraphics[width=1\textwidth]{img/ballpool}
  \end{columns}
  \vspace{.6cm}
  \begin{columns}
    \column{0.1\textwidth}
    \includegraphics[width=1\textwidth]{img/ballhand}
    \column{0.9\textwidth} A \structure{sample} is a subset of a
    population. ``A sample is chosen to make inferences about the
    population by examining or measuring the elements in the sample''
  \end{columns}
  \vspace{.6cm}
  \begin{columns}
    \column{0.9\textwidth} An \structure{observation} is a single
      element of a given sample, an individual data point. An
      observation can also be considered as a sample of size one.
      \column{0.1\textwidth}
      \includegraphics[width=1\textwidth]{img/greenball}
  \end{columns}

  \vspace{2cm}

  \rule{\textwidth}{0.4pt}
  \smaller{Glossary of statistical terms: \url{http://www.statistics.com/glossary}}
\end{frame}

\begin{frame}
  \frametitle{Population, Sample and Observation} 

  \begin{exampleblock}{Let's remember Alice and Bob's experiments}
    Alice and Bob build spam filter programs. They test their programs
    by counting how many spam the system catches in a day.
  \end{exampleblock}


  \begin{block}{Observation}
    \only<2>{
    If we count the number of spam caught by a system in one day, that
    is \structure{one observation}.

    \medskip
    
    If we count the number of spam caught by a system another day,
    that is \structure{a second observation}}
  \end{block}

  \begin{block}{Sample}
    
    \only<3>{If we count the number os spam caught every day for a
      week, we will have seven observations. That is a
      \structure{Sample}}
  \end{block}
  
  \begin{block}{Population}
    \only<4>{
      If we know ALL possible results for ALL possible days, that is the \structure{Population}
      
      \medskip
      
      In practice, it is \alert{usually impossible to KNOW} the
      population, but we want to learn \alert{as much as possible}
      from it, by observing samples.
    }
  \end{block}


\end{frame}

\section{Point and Interval Estimates}
\subsection{Concepts}

\begin{frame}
  \frametitle{Point and Interval Estimates} 

  Two central concepts of \structure{Statistical Inference} are
  \structure{point estimators} and \structure{statistical intervals}

  \medskip

  Both terms refer to the idea of using information obtained from a
  \structure{sample} to infer values about parameters of the
  \structure{population}.

  \vfill

  \begin{itemize}
    \item \structure{Point Estimate}: Estimate a value for a given population parameter
    \item \structure{Statistical Interval}: Estimate a interval of
      possible/probable values for a given population parameter;
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Point Estimates, Statistics, and Sampling distributions}
  
  Suppose one wants to obtain a point estimate for the mean of a given
  population. We take a sample of the population, and calculate the
  mean of that sample.

  \medskip

  However, a random sample from a population results in a random
  variable! Any function of the sample - any \emph{statistic} - is
  also a random variable.

  \medskip

  This means that statistics calculated from samples will also have
  their own probability distributions, called \structure{sampling
    distributions}.
  
  \begin{center}
    \includegraphics[width=0.3\textwidth]{img/meanestimator}
  \end{center}
  
  \rule{\textwidth}{0.4pt}
  {\tiny See D.W. Stockburger: \url{http://www.psychstat.missouristate.edu/introbook/sbk19.htm}}
\end{frame}

\begin{frame}[singleframe,fragile]
  \frametitle{I heard you like statistics!}
  {\small
  \begin{block}{}
    \begin{columns}
      
      \column{0.7\textwidth} 
      So in order to specify parameters of the population (such as
      means, deviation, etc), we draw a random sample and calculate the
      parameters from it.  
      \medskip

      But because the sample is random, the parameter calculated from
      the sample will also have its own statistics!
      \column{0.2\textwidth}
      \includegraphics[width=.8\textwidth]{img/yodawg}      
    \end{columns}
  \end{block}
  \begin{exampleblock}{Everything is easier with an R example}
\begin{verbatim}
> population <- rnorm(100) # Pretend you don't know this!
> x1 <- sample(population,5)
> x2 <- sample(population,5)
> x3 <- sample(population,5)
> x1
[1]  0.6028260  0.1333065  1.1145946 -0.8675467 -0.4329469
> c(pop=mean(population),x1=mean(x1),x2=mean(x2),x3=mean(x3))
        pop          x1          x2          x3 
 0.05722922  0.11004669 -0.10459150  0.12630965
> c(mean(c(mean(x1),mean(x2),mean(x3))),sd(c(mean(x1),mean(x2),mean(x3))))
[1] 0.04392161 0.12887292
\end{verbatim}
  \end{exampleblock}
}
\end{frame}

\subsection{Point Estimators}
\begin{frame}
  \frametitle{Point Estimators}

  A \structure{Point Estimator} is a statistic which provides the
  value of maximum plausibility for a given (unknown) population
  parameter $\theta$.

  \medskip

  Consider a random variable $X$ distributed according to a given
  $f(X|\theta)$ (a population which distribution is controlled by this
  parameter)

  \medskip

  Now consider also a random sample from this variable:\\
  $x = \{x_1,x_2,\dots,x_N\}$;

  \medskip

  A given function $\hat{\Theta} = h(x)$ is called a \emph{point
    estimator} of the parameter $\theta$, and a value returned by this
  function for a given sample is referred to as a \emph{point estimate
    $\hat{\theta}$} of the parameter.
  
  \medskip

  \begin{block}{What does this mean?}
    A \structure{Point Estimator} is a function that, given a sample,
    generates an estimated parameter for the distribution from which
    the sample was obtained. 
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Point Estimators} 

  Point estimation problems arise frequently in all areas of science
  and engineering, whenever there is a need for estimating a parameter
  of a population:
  \begin{itemize}
    \item The population mean, $\mu$;
    \item The population variange, $\sigma^2$;
    \item a population proportion, $p$;
    \item the difference in the means of two populations, $\mu_1 - \mu_2$;
    \item etc...
  \end{itemize}

  For each cases (and many others) there are multiple ways of
  performing the estimation task. We choose the estimators based on
  its statistics.
  
  {\smaller
  \begin{exampleblock}{Multiple estimators?}
    We always consider only one definition for estimators (e.g., the
    mean). But we can be creative and invent others!

    \begin{equation*}
      \mu = \sum^N_{i=0}\frac{x_i}{N}
    \end{equation*}
    \begin{equation*}
      \mu' = \frac{max(x) - min(x)}{2}
    \end{equation*}
  \end{exampleblock}}
\end{frame}

\begin{frame}
  \frametitle{Evaluating Estimators} 

  A good estimator should consistently generate estimates that are
  close to the real value of the parameter $\theta$.

  \bigskip

  We say that an estimator $\hat{Theta}$ is \structure{unbiased} for a parameter $\theta$ if:
  \begin{equation*}
    E[\hat{\Theta}] = \theta
  \end{equation*}
  or, equivalently:
  \begin{equation*}
    E[\hat{\Theta}] - \theta = 0.
  \end{equation*}

  \bigskip

  The difference $E[\hat{\Theta}] - \theta$ is referred as the
  \structure{bias} of an estimator.

\end{frame}

\begin{frame}
  \frametitle{Evaluating Estimators}
  The usual estimators for mean and variance are unbiased estimators;

  Let $x_1,\dots,x_N$ be a random sample from a given population $X$,
  which is characterized by its mean $\mu$ and variance $\sigma^2$. In
  this situation, it is possible to show that:
  \begin{equation*}
    E[\bar{x}] = E[\frac{1}{N}\sum_{i = 1}^Nx_i] = \mu
  \end{equation*}
  and:
  \begin{equation*}
    E[s^2] = E[\frac{1}{N-1}\sum^N_{i=1}(x_i-\bar{x})^2] = \sigma^2
  \end{equation*}

  \vspace{2cm}

  \rule{\textwidth}{0.4pt}
  {\tiny See this link for an example proof:
    \url{http://isites.harvard.edu/fs/docs/icb.topic515975.files/Proof\%20that\%20Sample\%20Variance\%20is\%20Unbiased.pdf}}
\end{frame}

\begin{frame}
  \frametitle{Evaluating Estimators (2)} 

  There usually exists more than one unbiased estimator for a
  parameter $\theta$. One way to choose which to use is to select the
  one with the smallest variance. This is generally called the
  \emph{minimal-variance unbiased estimator} (MVUE).
  \begin{center}
    \includegraphics[width=.6\textwidth]{img/ubes-var}
  \end{center}
  MVUE have the ability of generating estimates $\hat{\theta}$ that
  are relatively close to the real value.
\end{frame}

% TODO: Think about adding slides 12-14 from campelo class 3 (or replace by an example?)

\subsection{The Central Limit Theorem}
\begin{frame}
  \frametitle{Distribution of samples} 

  Even for an arbitrary population, the sampling distribution of means
  tends to be approximately normal (with $E[\bar{x}] = \mu$ and $s_{\bar{x}} = \sigma^2/N$

  \vfill

  \begin{alertblock}{Warning! Maths!}
    More generally, let $x_1,\dots,x_n$ be a sequence of independent
    and identically distributed (\structure{iid}) random variables,
    with mean $\mu$ and finite variance $\sigma^2$. Then:
    \begin{equation*}
      z_n = \frac{\sum^n_{i=1}(x_i)-n\mu}{\sqrt{n\sigma^2}}
    \end{equation*}
    is distributed approximately as a standard normal variable. That is, $z_n ~ N(0,1)$
    
    \medskip
    
    This is the \structure{Central Limit Theorem}.
  \end{alertblock}
\end{frame}

\begin{frame}[singleslide,fragile]
  \frametitle{Example of the Central Limit Theorem}
  
  \includegraphics[width=.45\textwidth]{img/CLT1}\hfill
  \includegraphics[width=.45\textwidth]{img/CLT5}

{\smaller
\begin{verbatim}
# Load the Teaching Demos library if you don't have it.
> install.packages("TeachingDemos")
> library(TeachingDemos)

> clt.examp()
> clt.examp(5)
\end{verbatim}}
\end{frame}

\begin{frame}
  \frametitle{Implications of the Central Limit Theorem} 

  The CLT is one of the most useful properties for statistical
  inference. The CLT allows the use of techniques based on the
  Gaussian distribution, even when the population under study is not
  normal.

  \bigskip
  
  For ``well-behaved'' distributions (continuous, symmetrical,
  unimodal) even small sample sizes are enough to justify invoking the
  CLT and using parametric techniques. 

  \bigskip

  For an interactive demonstration of the CLT, check:
  \url{http://drwho.cpdee.ufmg.br:3838/CLT/}
\end{frame}

% TODO: Add a numerical example of point estimators using R and some data

\begin{frame}
  \begin{center}
    mini-break, questions?
  \end{center}
\end{frame}


\section{Interval Estimators}
\subsection{Introduction}
\begin{frame}
  \frametitle{Statistical Intervals} 

  Statistical Intervals are important in quantifying the uncertainty
  associated to a given estimate;

  \bigskip
  
  % TODO: use a more relevant example to the students
  \begin{exampleblock}{Example: Coaxial cable factory}
    A coaxial cable manufacturing operation produces cables with a
    target resistance of 50$\Omega$ and a standard deviation of
    2$\Omega$. Assume that the resistance values of the cables
    produced can be well modeled by a normal distribution.
  \end{exampleblock}
  \begin{exampleblock}{}
    Suppose that we take a sample of $N = 25$ cables produced, and the
    sample mean is $\bar{x} = 48$. Given the variability of the
    sample, it is likely that this value is not exactly the true value
    $\mu$. 

    \medskip

    \structure{How can we quantify the uncertainty of this estimate?}
  \end{exampleblock}
\end{frame}

\begin{frame}
  \frametitle{Definition of Statistical Intervals}
  \structure{Statistical Intervals} define regions that are likely to
  contain the true value of an estimated parameter.
 
  \medskip
  
  More formally, it is generally possible to quantify the level of
  uncertainty associated with the estimation, which allows the
  derivation of sound \structure{conclusions at predefined levels of
    certainty}.

  \begin{exampleblock}{Example}
    We estimate that the value of the mean of this population is
    between 5.3 and 7.8, and we have a 95\% confidence on the method
    used to generate this interval.
  \end{exampleblock}

  The most common types of interval are:
  \begin{enumerate}
  \item Confidence Intervals;
  \item Tolerance Intervals;
  \item Prediction Intervals;
  \end{enumerate}
\end{frame}

\subsection{Interval definitions}
\begin{frame}
  \frametitle{Confidence Intervals} 

  Confidence Intervals quantify the degree of uncertainty associated
  with the estimation of the population parameter, such as the mean or
  the variance.

  \begin{block}{Definition}
    ``The interval that contains the true value of a given population
    with a confidence level of $100(1-\alpha)$''
  \end{block}
  
  \bigskip
  
  \begin{itemize}
    \item \alert{Wrong:} ``there is a 95\% chance that the interval
      contains the true population mean'':
    \item \structure{Right:} ``The method used to derive the interval
      has a hit rate of 95\%'' - i.e., the interval generated has a
      95\% chance of capturing the true population parameter;
  \end{itemize}

  \medskip

  It is easier to understand if you think about confidence in the {\bf
    method}, not in the interval.
\end{frame}

\begin{frame}[singleslide,fragile]
  \frametitle{Example, 100 $CI_{.95}$ for a sample of 25 observations}
  \begin{center}
    \includegraphics[width=.9\textwidth]{img/CIs}
  \end{center}
  \medskip
\begin{verbatim}
> library(TeachingDemos)
> run.ci.examp()
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{CI on the Mean of a Normal Variable} 
  The two-sided $CI_{(1-\alpha)}$ for the mean of a normal population
  with known variance $\sigma^2$ is given by:
  \begin{equation*}
    \bar{x} - \frac{\sigma}{\sqrt{N}}Z_{(\alpha/2)} \leq \mu \leq \bar{x} + \frac{\sigma}{\sqrt{N}}Z_{(\alpha/2)}
  \end{equation*}
  where $(1-\alpha)$ is the confidence level and $z_{(\alpha/2)}$ is
  the $(1-\alpha/2)$-quantile of the standard normal distribution.

  For the more usual case with an unknown variance,
  \begin{equation*}
    \bar{x} - \frac{s}{\sqrt{N}}t_{(\alpha/2;N-1)} \leq \mu \leq \bar{x} + \frac{s}{\sqrt{N}}t_{(\alpha/2;N-1)} 
  \end{equation*}
  where $t_{(\alpha/2;N-1)}$ is the corresponding quantile of the $t$
  distribution with $N-1$ degrees of freedom.
\end{frame}

\begin{frame}
  \frametitle{CI on the Variance of a Normal Variable} 
  In the same way, a two-sided confidence interval on the variance of
  a normal variable can be easily calculated:
  \begin{equation*}
    \frac{(N-1)s^2}{\chi^2_{\alpha/2;N-1}} \leq \sigma^2 \leq \frac{(N-1)s^2}{\chi^2_{1-\alpha/2;N-1}}
  \end{equation*}
  where $\chi^2_{\alpha/2;N-1}$ and $\chi^2_{1-\alpha/2;N-1}$ are the
  upper and lower $(\alpha/2)$-quantiles of the $\chi^2$ distribution
  with $N-1$ degrees of freedom.
\end{frame}

\begin{frame}[singleslide,fragile]
  \frametitle{Calculating the CI with R} 

  Remember that we don't want to do all these calculations by hand! It
  is important to understand what they mean, but in practice you will
  do something like this:
  
{\small
\begin{verbatim}
> population <- rnorm(5000) # our hypotethical population
> sample.size = 20
> x1 <- sample(population,sample.size) # replace with experiment

> mean.estimator <- mean(x1)
> sd.estimator <- sd(x1)

> left <- mean.estimator - (sd.estimator/sqrt(sample.size))*qt(0.95,df=sample.size-1)
> right <- mean.estimator + (sd.estimator/sqrt(sample.size))*qt(0.95,df=sample.size-1)

> c(left,right)
[1] -0.5218866  0.3356534
\end{verbatim}}
%TODO: Add another R example with real data
\end{frame}

\begin{frame}[singleslide,fragile]
  \frametitle{What if we want a smaller Interval?} 

  One way to decrease the size of the confidence interval, without
  losing confidence, is increasing the size of a sample. This has its
  own problems which we will see in the future (e.g. cost of
  sampling).
  
{\small
\begin{verbatim}
> population <- rnorm(5000) # our hypotethical population
> sample.size = 100 # INCREASED SAMPLE SIZE
> x1 <- sample(population,sample.size) # replace with experiment

> mean.estimator <- mean(x1)
> sd.estimator <- sd(x1)

> left <- mean.estimator - (sd.estimator/sqrt(sample.size))*qt(0.95,df=sample.size-1)
> right <- mean.estimator + (sd.estimator/sqrt(sample.size))*qt(0.95,df=sample.size-1)

> c(left,right)
[1] -0.1163320  0.2011607 # DECREASED CONFIDENCE INTERVAL
\end{verbatim}}
%TODO: Add another R example with real data
\end{frame}

\begin{frame}
  \frametitle{Tolerance Intervals} 

  ``A tolerance interval is a {\bf enclosure} interval for a specified
  proportion of the samplede population, not its mean or standard
  deviation. For a specified confidence level, you may want to
  determine lower and upper bounds such that a given percent of the
  population is contained within them.''

  \hfill{\tiny J.G. Ramirez:
    \url{https://www.sas.com/resources/whitepaper/wp_4430.pdf}}

  \begin{center}
    90\% enclosure of a N(50,2) population
    \includegraphics[width=0.8\textwidth]{img/enclosure}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Tolerance Intervals} 
  The common practice in engineering of defining specification limits
  by adding $\pm 3\sigma$ to a given estimate of the mean arises from
  this definition - for a normally-distributed population,
  approximately 99.75\% of the observations will fall within these
  limits.
  
  However, in most cases the true variance is unknown. So we have to
  use its estimate, $s^2$, and compensate for the uncertainty in this
  estimation. The two-sided tolerance interval is given as: 
  \begin{equation*}
    \bar{x} \pm \sqrt{(N-1)(N+z^2_{(\alpha/2)}}{N_{\chi^2_{(\gamma;N-1)}}}
  \end{equation*}
  In which $\gamma$ is the proportion of the population to be
  enclosed, and $1-\alpha$ is the desired confidence level for the
  interval.
\end{frame}

\begin{frame}[singleslide,fragile]
  \frametitle{Another Example} 
  We are bulding a program that should finish its operation in between
  10 and 30$\mu s$. An initial analysis would be to run the programs a
  few times to calculate the tolerance interval for its running time.

{\smaller
\begin{verbatim}
> runtime <- c(14.92869, 13.65345, 14.63093, 14.38412, 
               14.98059, 13.92460, 14.81254, 14.26117
               13.31676, 19.80000)
> df <- length(runtime)
> prop <- 0.9
> conf <- 0.95
> spread <- sqrt(((df-1)*(df+qnorm(conf/2)^2))/(df*qchisq(prop,df=df-1)))
> left <- mean(runtime) - spread
> right <- mean(runtime) + spread
> c(left,right)
[1] 14.08623 15.64972
\end{verbatim}}
% TODO: check that this calculation of tolerance interval is correct
\end{frame}

\begin{frame}
  \frametitle{Prediction Intervals} 
  Prediction intervals quantify the uncertainty associated with
  forecasting the value of a future observation;
  
  \medskip

  Essentially, one is intersted in obtaining an interval within which
  he or she can declare that the next observation will fall with a
  given probability;

  \medskip

  For a normal distribution, we have:
  \begin{equation*}
    \bar{x} - t_{(\alpha/2;N-1)}s/sqrt{1+\frac{1}{N}} \leq X_{N+1} \leq \bar{x} + t_{(\alpha/2;N-1)}s/sqrt{1+\frac{1}{N}}
  \end{equation*}
  which is similar to the confidence interval for the mean, but adding
  1 to the term within the square root to account for the prediction
  noise.
\end{frame}

\begin{frame}
  \frametitle{Wrapping up} 
  Statistical intervals quantify the uncertainty associated with
  different aspects of estimation;

  \bigskip

  Reporting intervals is always better than point estimates, as it
  provides to you (and your readers) the necessary information to
  quantify the location and spread of your estimated values;

  \bigskip

  The correct interpretation is a little tricky, but it is essential
  in order to derive the correct conclusions based on the statistical
  interval of interest.

  \bigskip

  \rule{\textwidth}{0.4pt}
  {\smaller
    Related reading:
    \begin{itemize}
      \item J.G. Ramirez, Statistical Intervals: Confidence, Prediction, Enclosure:\\
        \url{https://www.sas.com/resources/whitepaper/wp_4430.pdf}
      \item D.C. Montgomery and G.C. Runger, ``Applied Statistics and
        Probability for Engineers'', chapter 8, 3rd Ed., Wiley 2005.
    \end{itemize}    
  }

\end{frame}

\section{Hypothesis Testing}
% TODO: Add campelo class 5

\section{End Notes}
\subsection{Homework 2}
\begin{frame}
  \frametitle{Homework 2}
  \begin{block}{Basic Data analysis}
    \begin{itemize}
    \item Load two data files related to your research into R data
      frames (experiment results, data sets, etc);
    \item Find out, for each data set: variables, their means,
      variances, maximum and minimum values;
    \item Plot relevant plots to characterize these data sets;
    \item Compare these data sets using statistical estimators (point
      estimators, interval estimators, etc)
    \item Describe your findings;
    \end{itemize}
  \end{block}
  \begin{block}{Submission materials}
    \begin{itemize}
    \item \structure{Files 1..n:} text files containing the data used;
    \item \structure{File n+1:} R file (text file) containing the
      tasks above; R file must contain comments explaining what each
      command block does.
    \end{itemize}
  \end{block}
  \rule{\textwidth}{0.4pt} {\tiny If you don't have useable data
    related to your research, you can use the following data sets:}
\end{frame}

\section{Final Notes}
\subsection{Image Information}
\begin{frame}
  \frametitle{Credits}
  {\smaller
  \begin{itemize}
    %TODO: Generate self image for pool of balls
  \item Pool of balls image: \url{http://goo.gl/y8doaN}
    %TODO: Generate self image for green ball
  \item Green ball: \url{http://goo.gl/Fb8z68}
    %TODO: Generate self image for MVUE (min-max/2 vs regular mean)
  \item MVUE image: D.C.Montgomery, G.C. Runger, ``Applied Statistics and Probability for Engineers'', Wiley 2003
  \end{itemize}
  \medskip
  \begin{block}{This lecture notes is a derived work of}
    Felipe Campelo (2015), ``Lecture Notes on Design and Analysis of Experiments''\\
    Online: \url{https://github.com/fcampelo/Design-and-Analysis-of-Experiments}
    Creative Commons BY-NC-SA 4.0.
  \end{block}}
\end{frame}

\end{document}
